# 고급 프로젝트 협업 일지 (1팀 박은서)

## 오늘 할 일
* 프로젝트 타임라인 구성하기
* 파일 관리방식 논의하기
* 데이터 읽어오는 작업 코드 작성하기
## 오늘 한 일
* 프로젝트 타임라인 구성하기
> 팀원들과 논의를 통해 중간발표 전의 분석 주제 선정, EDA 관련 일정을 구체화했다. 기간을 타이트하게 잡기보다는 넉넉하게 데드라인을 정해놓고 유동적으로 일정을 쓰기로 정했다. EDA 내용도 수시로 공유하고, 오후 4-5시 사이에는 반드시 당일 진행한 내용을 공유하도록 설정했다.
* 파일 관리방식 논의하기
> 로우 데이터는 현승님께서 로컬에 다운받아 GCS에 올려주셨기에, 해당 파일을 다른 팀원들이 VSCODE 등의 환경으로 읽어오기로 결정했다. 이후 분석에 돌입하게 된다면 버전 관리를 위해 GIT의 BRANCH를 활용하기로 대화를 나눴다.
* 데이터 읽어오는 작업 코드 작성하기
> 현승님께서 발급해주신 JSON KEY와 종헌님의 코드를 참고해 와일드카드 패턴을 이용하여 여러 parquet 파일을 한 번에 조회해 리스트에 저장하는 코드를 만들었다.
## 내일 할 일
* 데이터 구조 파악 및 EDA
* 주제 아이디에이션
## Codes
```ruby
import pandas as pd
import gcsfs
import os

# 서비스 계정 키 경로
key_path = '/home/silverwest0822/Apache_Airflow/sprintda07-hyunseung-b549fc3c2f43.json'

# gcsfs 파일시스템 객체
fs = gcsfs.GCSFileSystem(token=key_path)

# 와일드카드로 GCS 파일 리스트 받기
file_list = fs.glob('gs://sprintda07-hyunseung/dataset/votes/accounts_*.parquet')
print("찾은 파일들:", file_list)

# 파일별 DataFrame을 저장할 딕셔너리
dfs = {}

# 각 파일 읽어서 개별 저장
for file_path in file_list:
    with fs.open(file_path, 'rb') as f:
        df = pd.read_parquet(f, engine='pyarrow')
        file_key = os.path.basename(file_path).replace('.parquet', '')  # 파일명만 키로 사용
        dfs[file_key] = df
        print(f"✅ {file_key} → shape: {df.shape}")

# 예시: 특정 DataFrame 사용
print(dfs['accounts_school'].head())
```
## Issues & Challenges
* 문제 : 로컬을 거치지 않고 드라이브에서 GCS로 옮기고 싶었는데, 과정이 너무 복잡해지고 오류도 많이 났다. 나름 시도해보다가 알게된 사실은 결국 로컬을 거치고야 만다는 사실이었다. 그래도 시도해본 것에 의의를 둔다.
## Notes & Ideas
* 아이디어 : 아직까지는 여유가 있어서 그런지 데이터를 불러들임에 있어 효율적인 방식이 무엇인지 충분히 고민해보는 시간을 가질 수 있었다. 결과적으로는 실패했지만 왜 안되는지 안 것도 성과라고 생각한다.
## Reflection
* 좋았던 점 : 팀원들이 소통에 매우 적극적이라 서로 의견을 나누고 문제를 해결하는 과정이 즐거웠다.
* 아쉬웠던 점 : 데이터 불러오는 코드를 더 빨리 완성했다면 구조 파악을 할 수 있었을텐데, 그 부분이 아쉽다.
